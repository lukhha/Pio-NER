{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/22 08:26:04 INFO mlflow.tracking.fluent: Experiment with name 'NER_Casestudy_Experiment2' does not exist. Creating a new experiment.\n",
      "/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2206b70dbc4249aba7bbd7fdf2c00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a290dcc2533440f6b4d66ac8594fdbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d5b2744b264db2af468fa344b3e3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Invalid parent directory '/Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns/.trash'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 163\u001b[0m\n\u001b[1;32m    152\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    153\u001b[0m     model,\n\u001b[1;32m    154\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m    160\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Start MLflow run\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m#     # Log metrics to MLflow\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m#     mlflow.log_metrics(results)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/tracking/fluent.py:374\u001b[0m, in \u001b[0;36mstart_run\u001b[0;34m(run_id, experiment_id, run_name, nested, tags, description, log_system_metrics)\u001b[0m\n\u001b[1;32m    370\u001b[0m     user_specified_tags[MLFLOW_RUN_NAME] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[1;32m    372\u001b[0m resolved_tags \u001b[38;5;241m=\u001b[39m context_registry\u001b[38;5;241m.\u001b[39mresolve_tags(user_specified_tags)\n\u001b[0;32m--> 374\u001b[0m active_run_obj \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_id_for_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_system_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# if `log_system_metrics` is not specified, we will check environment variable.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     log_system_metrics \u001b[38;5;241m=\u001b[39m MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/tracking/client.py:332\u001b[0m, in \u001b[0;36mMlflowClient.create_run\u001b[0;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_run\u001b[39m(\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    283\u001b[0m     experiment_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     run_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Run:\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    Create a :py:class:`mlflow.entities.Run` object that can be associated with\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    metrics, parameters, artifacts, etc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m        status: RUNNING\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py:132\u001b[0m, in \u001b[0;36mTrackingServiceClient.create_run\u001b[0;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Extract user from tags\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# This logic is temporary; the user_id attribute of runs is deprecated and will be removed\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# in a later release.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m user_id \u001b[38;5;241m=\u001b[39m tags\u001b[38;5;241m.\u001b[39mget(MLFLOW_USER, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_current_time_millis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mRunTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py:651\u001b[0m, in \u001b[0;36mFileStore.create_run\u001b[0;34m(self, experiment_id, user_id, start_time, tags, run_name)\u001b[0m\n\u001b[1;32m    649\u001b[0m mkdir(run_dir, FileStore\u001b[38;5;241m.\u001b[39mARTIFACTS_FOLDER_NAME)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_uuid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_run(run_id\u001b[38;5;241m=\u001b[39mrun_uuid)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py:1001\u001b[0m, in \u001b[0;36mFileStore.set_tag\u001b[0;34m(self, run_id, tag)\u001b[0m\n\u001b[1;32m    999\u001b[0m _validate_run_id(run_id)\n\u001b[1;32m   1000\u001b[0m _validate_tag_name(tag\u001b[38;5;241m.\u001b[39mkey)\n\u001b[0;32m-> 1001\u001b[0m run_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_run_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m check_run_is_active(run_info)\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_run_tag(run_info, tag)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py:681\u001b[0m, in \u001b[0;36mFileStore._get_run_info\u001b[0;34m(self, run_uuid)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_run_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, run_uuid):\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m    Note: Will get both active and deleted runs.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m     exp_id, run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_run_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_uuid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    684\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_uuid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m, databricks_pb2\u001b[38;5;241m.\u001b[39mRESOURCE_DOES_NOT_EXIST\n\u001b[1;32m    685\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py:581\u001b[0m, in \u001b[0;36mFileStore._find_run_root\u001b[0;34m(self, run_uuid)\u001b[0m\n\u001b[1;32m    579\u001b[0m _validate_run_id(run_uuid)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_root_dir()\n\u001b[0;32m--> 581\u001b[0m all_experiments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_active_experiments(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_deleted_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m experiment_dir \u001b[38;5;129;01min\u001b[39;00m all_experiments:\n\u001b[1;32m    583\u001b[0m     runs \u001b[38;5;241m=\u001b[39m find(experiment_dir, run_uuid, full_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py:268\u001b[0m, in \u001b[0;36mFileStore._get_deleted_experiments\u001b[0;34m(self, full_path)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_deleted_experiments\u001b[39m(\u001b[38;5;28mself\u001b[39m, full_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlist_subdirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrash_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/utils/file_utils.py:159\u001b[0m, in \u001b[0;36mlist_subdirs\u001b[0;34m(dir_name, full_path)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_subdirs\u001b[39m(dir_name, full_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    Equivalent to UNIX command:\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m      ``find $dir_name -depth 1 -type d``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    :return: list of all directories directly under 'dir_name'\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlist_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/utils/file_utils.py:144\u001b[0m, in \u001b[0;36mlist_all\u001b[0;34m(root, filter_func, full_path)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mList all entities directly under 'dir_name' that satisfy 'filter_func'\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m:return: list of all files or directories that satisfy the criteria.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(root):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parent directory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m matches \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(root) \u001b[38;5;28;01mif\u001b[39;00m filter_func(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, x))]\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matches] \u001b[38;5;28;01mif\u001b[39;00m full_path \u001b[38;5;28;01melse\u001b[39;00m matches\n",
      "\u001b[0;31mException\u001b[0m: Invalid parent directory '/Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns/.trash'"
     ]
    }
   ],
   "source": [
    "#import evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# Ensure MLflow directory exists\n",
    "mlruns_dir = '/Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns'\n",
    "if not os.path.exists(mlruns_dir):\n",
    "    os.makedirs(mlruns_dir)\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri('file:///Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns')\n",
    "\n",
    "#experiment_id = mlflow.create_experiment('NER_Casestudy_Experiment')\n",
    "\n",
    "# Create or get the experiment\n",
    "experiment_name = \"NER_Casestudy_Experiment2\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "#mlflow.transformers.autolog()\n",
    "\n",
    "\n",
    "# Load the dataset with a specified encoding\n",
    "file_path = '/Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/data/ner_dataset.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "\n",
    "data=data.head(1000)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Group the data by sentences\n",
    "data['Sentence #'] = data['Sentence #'].ffill()  # Fill forward to propagate sentence IDs\n",
    "sentences = data.groupby('Sentence #').apply(lambda s: [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                                                      s['POS'].values.tolist(),\n",
    "                                                                                      s['Tag'].values.tolist())])\n",
    "# Convert the groupby object to a list of sentences\n",
    "sentences = [s for s in sentences]\n",
    "\n",
    "# Split the dataset into training, validation, and test sets (20% for test)\n",
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.20, random_state=42)\n",
    "train_sentences, val_sentences = train_test_split(train_sentences, test_size=0.25, random_state=42)  # 0.25 * 0.80 = 0.20\n",
    "\n",
    "# Convert to Hugging Face Datasets format\n",
    "def convert_to_dict(sentences):\n",
    "    words = [[word for word, pos, tag in sentence] for sentence in sentences]\n",
    "    pos_tags = [[pos for word, pos, tag in sentence] for sentence in sentences]\n",
    "    ner_tags = [[tag for word, pos, tag in sentence] for sentence in sentences]\n",
    "    return {\"tokens\": words, \"pos_tags\": pos_tags, \"ner_tags\": ner_tags}\n",
    "\n",
    "train_data = convert_to_dict(train_sentences)\n",
    "val_data = convert_to_dict(val_sentences)\n",
    "test_data = convert_to_dict(test_sentences)\n",
    "\n",
    "# Create a dataset dictionary\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict(train_data),\n",
    "    'validation': Dataset.from_dict(val_data),\n",
    "    'test': Dataset.from_dict(test_data)\n",
    "})\n",
    "\n",
    "# Define unique tags\n",
    "unique_tags = list(set(tag for doc in dataset_dict['train']['ner_tags'] for tag in doc))\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "# Tokenizer\n",
    "model_checkpoint = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(tag2id[label[word_idx]] if True else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Model\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(unique_tags), ignore_mismatched_sizes=True)\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, len(unique_tags))\n",
    "model.num_labels = len(unique_tags)\n",
    "\n",
    "# Metrics\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    #\"test-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    use_mps_device=True,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "#     # Log metrics to MLflow\n",
    "#     mlflow.log_metrics(results)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Log the model artifact\n",
    "    trainer.save_model(os.path.join(\"results\", \"model\"))\n",
    "    tokenizer.save_pretrained(os.path.join(\"results\", \"model\"))\n",
    "\n",
    "    mlflow.log_artifacts(\"results/model\")\n",
    "\n",
    "\n",
    "    # Log other artifacts if needed\n",
    "    # For example, logging training args\n",
    "    with open(\"results/training_args.bin\", \"wb\") as f:\n",
    "        torch.save(args, f)\n",
    "    mlflow.log_artifact(\"results/training_args.bin\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the model\n",
    "tuned_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\",device=device)\n",
    "input_example = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\n",
    "# ner_results = ner_pipeline(sample_text)\n",
    "# print(ner_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Infer the model signature, including a representative input, the expected output, and the parameters that we would like to be able to override at inference time.\n",
    "signature = mlflow.models.infer_signature(\n",
    "    [\"This is a test!\", \"And this is also a test.\"],\n",
    "    mlflow.transformers.generate_signature_output(\n",
    "        tuned_pipeline, [\"This is a test response!\", \"So is this.\"]\n",
    "    ),\n",
    "    #params=model_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log the pipeline to the existing training run\n",
    "with mlflow.start_run(run_id=run.info.run_id):\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=tuned_pipeline,\n",
    "        #artifact_path=\"fine_tuned\",\n",
    "        artifact_path=\"\",\n",
    "        signature=signature,\n",
    "        input_example=[\"Pass in a string\", \"And have it mark as spam or not.\"],\n",
    "        #model_config=model_config,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/22 08:23:43 INFO mlflow.transformers: 'runs:/3efd0ad246504b3cb406dc92db40a9a9/' resolved as 'file:///Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns/183369609527481520/3efd0ad246504b3cb406dc92db40a9a9/artifacts'\n",
      "2024/06/22 08:23:44 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_2',\n",
       "  'score': 0.36942142,\n",
       "  'index': 1,\n",
       "  'word': 'Want',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.46124288,\n",
       "  'index': 2,\n",
       "  'word': 'to',\n",
       "  'start': 5,\n",
       "  'end': 7},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.45243514,\n",
       "  'index': 3,\n",
       "  'word': 'learn',\n",
       "  'start': 8,\n",
       "  'end': 13},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.46696356,\n",
       "  'index': 4,\n",
       "  'word': 'how',\n",
       "  'start': 14,\n",
       "  'end': 17},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.47794893,\n",
       "  'index': 5,\n",
       "  'word': 'to',\n",
       "  'start': 18,\n",
       "  'end': 20},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.45272633,\n",
       "  'index': 6,\n",
       "  'word': 'make',\n",
       "  'start': 21,\n",
       "  'end': 25},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.4288166,\n",
       "  'index': 7,\n",
       "  'word': 'MI',\n",
       "  'start': 26,\n",
       "  'end': 28},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.5321631,\n",
       "  'index': 8,\n",
       "  'word': '##LL',\n",
       "  'start': 28,\n",
       "  'end': 30},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.4850414,\n",
       "  'index': 9,\n",
       "  'word': '##ION',\n",
       "  'start': 30,\n",
       "  'end': 33},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.49362966,\n",
       "  'index': 10,\n",
       "  'word': '##S',\n",
       "  'start': 33,\n",
       "  'end': 34},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.44800916,\n",
       "  'index': 11,\n",
       "  'word': 'with',\n",
       "  'start': 35,\n",
       "  'end': 39},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.49720877,\n",
       "  'index': 12,\n",
       "  'word': 'no',\n",
       "  'start': 40,\n",
       "  'end': 42},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.4342368,\n",
       "  'index': 13,\n",
       "  'word': 'effort',\n",
       "  'start': 43,\n",
       "  'end': 49},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.44289124,\n",
       "  'index': 14,\n",
       "  'word': '?',\n",
       "  'start': 49,\n",
       "  'end': 50},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.45784566,\n",
       "  'index': 15,\n",
       "  'word': 'C',\n",
       "  'start': 51,\n",
       "  'end': 52},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.4520828,\n",
       "  'index': 16,\n",
       "  'word': '##lick',\n",
       "  'start': 52,\n",
       "  'end': 56},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.42845306,\n",
       "  'index': 17,\n",
       "  'word': 'H',\n",
       "  'start': 57,\n",
       "  'end': 58},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.43074805,\n",
       "  'index': 18,\n",
       "  'word': '##ER',\n",
       "  'start': 58,\n",
       "  'end': 60},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.446294,\n",
       "  'index': 19,\n",
       "  'word': '##E',\n",
       "  'start': 60,\n",
       "  'end': 61},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.45215875,\n",
       "  'index': 20,\n",
       "  'word': 'now',\n",
       "  'start': 62,\n",
       "  'end': 65},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.4364935,\n",
       "  'index': 21,\n",
       "  'word': '!',\n",
       "  'start': 65,\n",
       "  'end': 66},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.4578805,\n",
       "  'index': 22,\n",
       "  'word': 'See',\n",
       "  'start': 67,\n",
       "  'end': 70},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.5567404,\n",
       "  'index': 23,\n",
       "  'word': 'for',\n",
       "  'start': 71,\n",
       "  'end': 74},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.4235678,\n",
       "  'index': 24,\n",
       "  'word': 'yourself',\n",
       "  'start': 75,\n",
       "  'end': 83},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.4570747,\n",
       "  'index': 25,\n",
       "  'word': '!',\n",
       "  'start': 83,\n",
       "  'end': 84},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.4023435,\n",
       "  'index': 26,\n",
       "  'word': 'G',\n",
       "  'start': 85,\n",
       "  'end': 86},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.60268885,\n",
       "  'index': 27,\n",
       "  'word': '##ua',\n",
       "  'start': 86,\n",
       "  'end': 88},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.44354826,\n",
       "  'index': 28,\n",
       "  'word': '##rant',\n",
       "  'start': 88,\n",
       "  'end': 92},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.4493608,\n",
       "  'index': 29,\n",
       "  'word': '##eed',\n",
       "  'start': 92,\n",
       "  'end': 95},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.5137036,\n",
       "  'index': 30,\n",
       "  'word': 'to',\n",
       "  'start': 96,\n",
       "  'end': 98},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.47535145,\n",
       "  'index': 31,\n",
       "  'word': 'make',\n",
       "  'start': 99,\n",
       "  'end': 103},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.49869004,\n",
       "  'index': 32,\n",
       "  'word': 'you',\n",
       "  'start': 104,\n",
       "  'end': 107},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.468379,\n",
       "  'index': 33,\n",
       "  'word': 'instantly',\n",
       "  'start': 108,\n",
       "  'end': 117},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.43758562,\n",
       "  'index': 34,\n",
       "  'word': 'rich',\n",
       "  'start': 118,\n",
       "  'end': 122},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.44241694,\n",
       "  'index': 35,\n",
       "  'word': '!',\n",
       "  'start': 122,\n",
       "  'end': 123},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.40085036,\n",
       "  'index': 36,\n",
       "  'word': 'Don',\n",
       "  'start': 124,\n",
       "  'end': 127},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.50644743,\n",
       "  'index': 37,\n",
       "  'word': \"'\",\n",
       "  'start': 127,\n",
       "  'end': 128},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.49638414,\n",
       "  'index': 38,\n",
       "  'word': 't',\n",
       "  'start': 128,\n",
       "  'end': 129},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.5047935,\n",
       "  'index': 39,\n",
       "  'word': 'miss',\n",
       "  'start': 130,\n",
       "  'end': 134},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.5109022,\n",
       "  'index': 40,\n",
       "  'word': 'out',\n",
       "  'start': 135,\n",
       "  'end': 138},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.47757018,\n",
       "  'index': 41,\n",
       "  'word': 'you',\n",
       "  'start': 139,\n",
       "  'end': 142},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.49831674,\n",
       "  'index': 42,\n",
       "  'word': 'could',\n",
       "  'start': 143,\n",
       "  'end': 148},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.47666076,\n",
       "  'index': 43,\n",
       "  'word': 'be',\n",
       "  'start': 149,\n",
       "  'end': 151},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.49511087,\n",
       "  'index': 44,\n",
       "  'word': 'a',\n",
       "  'start': 152,\n",
       "  'end': 153},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.49714783,\n",
       "  'index': 45,\n",
       "  'word': 'winner',\n",
       "  'start': 154,\n",
       "  'end': 160},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.41627446,\n",
       "  'index': 46,\n",
       "  'word': '!',\n",
       "  'start': 160,\n",
       "  'end': 161}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load our saved model in the native transformers format\n",
    "loaded = mlflow.transformers.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "# Define a test example that we expect to be classified as spam\n",
    "validation_text = (\n",
    "    \"Want to learn how to make MILLIONS with no effort? Click HERE now! See for yourself! Guaranteed to make you instantly rich! \"\n",
    "    \"Don't miss out you could be a winner!\"\n",
    ")\n",
    "\n",
    "# validate the performance of our fine-tuning\n",
    "loaded(validation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m,}\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#http_data = test_df.to_json(orient='split')\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m http_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;28mlist\u001b[39m(\u001b[43mX_test\u001b[49m[\u001b[38;5;241m0\u001b[39m])]}\n\u001b[1;32m     10\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39murl, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(http_data))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "host = 'localhost'\n",
    "port = '1234'\n",
    "url = f'http://{host}:{port}/invocations'\n",
    "headers = {'Content-Type': 'application/json',}\n",
    "#http_data = test_df.to_json(orient='split')\n",
    "http_data={'inputs':[list(X_test[0])]}\n",
    "\n",
    "r = requests.post(url=url, headers=headers, data=json.dumps(http_data))\n",
    "print(f'Predictions: {r.text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
