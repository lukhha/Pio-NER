{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Lazy huggingface pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9886593, 'word': 'Kovácsné Nagy Erzsébet', 'start': 2, 'end': 24}, {'entity_group': 'ORG', 'score': 0.9960921, 'word': 'Nokián', 'start': 49, 'end': 55}, {'entity_group': 'LOC', 'score': 0.99254215, 'word': 'Németország', 'start': 69, 'end': 80}, {'entity_group': 'PER', 'score': 0.99941576, 'word': 'Kovács Péter', 'start': 93, 'end': 105}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(task=\"ner\", model=\"NYTK/named-entity-recognition-nerkor-hubert-hungarian\")\n",
    "input_text = \"A Kovácsné Nagy Erzsébet nagyon jól érzi magát a Nokiánál, azonban a Németországból érkezett Kovács Péter nehezen boldogul a beilleszkedéssel.\"\n",
    "\n",
    "print(ner(input_text, aggregation_strategy=\"simple\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Huggingface Transformer (Pre trained BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "pre_trained_model=\"dslim/bert-base-NER\"\n",
    "\n",
    "#model=\"nlpso/m3_hierarchical_ner_ocr_ptrn_cmbert_iob2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(pre_trained_model)\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1422, 1271, 1110, 14326, 1105, 146, 1686, 1107, 3206, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOB to IOB2 convertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IOB1:  O I I B I\n",
    "IOB2:  O B I B I\n",
    "\"\"\"\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def iob2(tags: List[str]):\n",
    "    \"\"\"\n",
    "    Check that tags have a valid IOB format.\n",
    "    Tags in IOB1 format are converted to IOB2.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "        split = tag.split('-')\n",
    "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
    "            return False\n",
    "        if split[0] == 'B':\n",
    "            continue\n",
    "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "        elif tags[i - 1][1:] == tag[1:]:\n",
    "            continue\n",
    "        else:  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFLOW initiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns/871896506229020652', creation_time=1719024964319, experiment_id='871896506229020652', last_update_time=1719024964319, lifecycle_stage='active', name='NER_Casestudy_Experiment2', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure MLflow directory exists\n",
    "mlruns_dir = '/Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns'\n",
    "if not os.path.exists(mlruns_dir):\n",
    "    os.makedirs(mlruns_dir)\n",
    "\n",
    "mlruns_trash_dir = '/Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns/.trash'\n",
    "if not os.path.exists(mlruns_trash_dir):\n",
    "    os.makedirs(mlruns_trash_dir)\n",
    "    \n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri('file:///Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns')\n",
    "\n",
    "#experiment_id = mlflow.create_experiment('NER_Casestudy_Experiment')\n",
    "\n",
    "# Create or get the experiment\n",
    "experiment_name = \"NER_Casestudy_Experiment2\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading out input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset with a specified encoding\n",
    "file_path = '/Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/data/ner_dataset.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trucating for quick experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    957\n",
       "Word            0\n",
       "POS             0\n",
       "Tag             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping records having null values for our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with NaN values\n",
    "data = data.dropna(subset=['Word','POS','Tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    957\n",
       "Word            0\n",
       "POS             0\n",
       "Tag             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the Tag Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter as C\n",
    "C(data['Tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructuring the data in desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the data by sentences\n",
    "data['Sentence #'] = data['Sentence #'].ffill()  # Fill forward to propagate sentence IDs\n",
    "sentences = data.groupby('Sentence #').apply(lambda s: [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                                                      s['POS'].values.tolist(),\n",
    "                                                                                      s['Tag'].values.tolist())])\n",
    "# Convert the groupby object to a list of sentences\n",
    "sentences = [s for s in sentences]\n",
    "\n",
    "# Split the dataset into training, validation, and test sets (20% for test)\n",
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.20, random_state=42)\n",
    "train_sentences, val_sentences = train_test_split(train_sentences, test_size=0.25, random_state=42)  # 0.25 * 0.80 = 0.20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Poor', 'JJ', 'O'),\n",
       "  ('residents', 'NNS', 'O'),\n",
       "  ('often', 'RB', 'O'),\n",
       "  ('complain', 'VBP', 'O'),\n",
       "  ('they', 'PRP', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('been', 'VBN', 'O'),\n",
       "  ('cheated', 'VBN', 'O'),\n",
       "  ('out', 'IN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('huge', 'JJ', 'O'),\n",
       "  ('riches', 'NNS', 'O'),\n",
       "  ('extracted', 'VBN', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('their', 'PRP$', 'O'),\n",
       "  ('tribal', 'JJ', 'O'),\n",
       "  ('lands', 'NNS', 'O'),\n",
       "  ('-', ':', 'O'),\n",
       "  ('where', 'WRB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('bulk', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('Nigeria', 'NNP', 'B-gpe'),\n",
       "  (\"'s\", 'POS', 'O'),\n",
       "  ('2.3', 'CD', 'O'),\n",
       "  ('million', 'CD', 'O'),\n",
       "  ('barrels', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('petroleum', 'NN', 'O'),\n",
       "  ('are', 'VBP', 'O'),\n",
       "  ('pumped', 'VBN', 'O'),\n",
       "  ('daily', 'RB', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('The', 'DT', 'O'),\n",
       "  ('European', 'NNP', 'B-org'),\n",
       "  ('Union', 'NNP', 'I-org'),\n",
       "  (',', ',', 'O'),\n",
       "  ('with', 'IN', 'O'),\n",
       "  ('U.S.', 'NNP', 'B-gpe'),\n",
       "  ('backing', 'NN', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('has', 'VBZ', 'O'),\n",
       "  ('threatened', 'VBN', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('refer', 'VB', 'O'),\n",
       "  ('Iran', 'NNP', 'B-gpe'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('U.N.', 'NNP', 'B-org'),\n",
       "  ('Security', 'NNP', 'I-org'),\n",
       "  ('Council', 'NNP', 'I-org'),\n",
       "  (',', ',', 'O'),\n",
       "  ('which', 'WDT', 'O'),\n",
       "  ('could', 'MD', 'O'),\n",
       "  ('impose', 'VB', 'O'),\n",
       "  ('sanctions', 'NNS', 'O'),\n",
       "  ('if', 'IN', 'O'),\n",
       "  ('it', 'PRP', 'O'),\n",
       "  ('finds', 'VBZ', 'O'),\n",
       "  ('Tehran', 'NNP', 'B-gpe'),\n",
       "  ('has', 'VBZ', 'O'),\n",
       "  ('violated', 'VBN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('Nuclear', 'NNP', 'B-art'),\n",
       "  ('Non-Proliferation', 'NNP', 'I-art'),\n",
       "  ('treaty', 'NN', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Mosul', 'NNP', 'B-geo'),\n",
       "  ('is', 'VBZ', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('largest', 'JJS', 'O'),\n",
       "  ('city', 'NN', 'O'),\n",
       "  ('north', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('Baghdad', 'NNP', 'B-geo'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('has', 'VBZ', 'O'),\n",
       "  ('long', 'RB', 'O'),\n",
       "  ('been', 'VBN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('stronghold', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('Sunni', 'NNP', 'B-org'),\n",
       "  ('militant', 'JJ', 'O'),\n",
       "  ('fighters', 'NNS', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Two', 'CD', 'O'),\n",
       "  ('Germans', 'NNS', 'B-gpe'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('four', 'CD', 'O'),\n",
       "  ('Nigerian', 'JJ', 'B-gpe'),\n",
       "  ('oil', 'NN', 'O'),\n",
       "  ('workers', 'NNS', 'O'),\n",
       "  ('were', 'VBD', 'O'),\n",
       "  ('kidnapped', 'VBN', 'O'),\n",
       "  ('by', 'IN', 'O'),\n",
       "  ('armed', 'JJ', 'O'),\n",
       "  ('militants', 'NNS', 'O'),\n",
       "  ('during', 'IN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('raid', 'NN', 'O'),\n",
       "  ('on', 'IN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('boat', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Nigeria', 'NNP', 'B-geo'),\n",
       "  (\"'s\", 'POS', 'O'),\n",
       "  ('southern', 'JJ', 'O'),\n",
       "  ('oil-rich', 'JJ', 'O'),\n",
       "  ('Delta', 'NNP', 'B-geo'),\n",
       "  ('region', 'NN', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Families', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('soldiers', 'NNS', 'O'),\n",
       "  ('killed', 'VBN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('conflict', 'NN', 'O'),\n",
       "  ('joined', 'VBD', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('protesters', 'NNS', 'O'),\n",
       "  ('who', 'WP', 'O'),\n",
       "  ('carried', 'VBD', 'O'),\n",
       "  ('banners', 'NNS', 'O'),\n",
       "  ('with', 'IN', 'O'),\n",
       "  ('such', 'JJ', 'O'),\n",
       "  ('slogans', 'NNS', 'O'),\n",
       "  ('as', 'IN', 'O'),\n",
       "  ('\"', '``', 'O'),\n",
       "  ('Bush', 'NNP', 'B-per'),\n",
       "  ('Number', 'NN', 'O'),\n",
       "  ('One', 'CD', 'O'),\n",
       "  ('Terrorist', 'NN', 'O'),\n",
       "  ('\"', '``', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('\"', '``', 'O'),\n",
       "  ('Stop', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('Bombings', 'NNS', 'O'),\n",
       "  ('.', '.', 'O'),\n",
       "  ('\"', '``', 'O')],\n",
       " [('U.S.', 'NNP', 'B-gpe'),\n",
       "  ('commanders', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('not', 'RB', 'O'),\n",
       "  ('explained', 'VBN', 'O'),\n",
       "  ('how', 'WRB', 'O'),\n",
       "  ('American', 'JJ', 'B-gpe'),\n",
       "  ('forces', 'NNS', 'O'),\n",
       "  ('will', 'MD', 'O'),\n",
       "  ('participate', 'VB', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('offensive', 'NN', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Thousands', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('demonstrators', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('marched', 'VBN', 'O'),\n",
       "  ('through', 'IN', 'O'),\n",
       "  ('London', 'NNP', 'B-geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('protest', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('war', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Iraq', 'NNP', 'B-geo'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('demand', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('withdrawal', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('British', 'JJ', 'B-gpe'),\n",
       "  ('troops', 'NNS', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('that', 'DT', 'O'),\n",
       "  ('country', 'NN', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('The', 'DT', 'O'),\n",
       "  ('provincial', 'JJ', 'O'),\n",
       "  ('governor', 'NN', 'O'),\n",
       "  ('must', 'MD', 'O'),\n",
       "  ('still', 'RB', 'O'),\n",
       "  ('sign', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('bill', 'NN', 'O'),\n",
       "  ('before', 'IN', 'O'),\n",
       "  ('it', 'PRP', 'O'),\n",
       "  ('becomes', 'VBZ', 'O'),\n",
       "  ('law', 'NN', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('step', 'NN', 'O'),\n",
       "  ('seen', 'VBN', 'O'),\n",
       "  ('only', 'RB', 'O'),\n",
       "  ('as', 'IN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('formality', 'NN', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('An', 'DT', 'O'),\n",
       "  ('official', 'NN', 'O'),\n",
       "  ('with', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('German', 'JJ', 'B-gpe'),\n",
       "  ('firm', 'NN', 'O'),\n",
       "  ('Bilfinger', 'NNP', 'B-org'),\n",
       "  ('Berger', 'NNP', 'I-org'),\n",
       "  (',', ',', 'O'),\n",
       "  ('Thomas', 'NNP', 'B-per'),\n",
       "  ('Horbach', 'NNP', 'I-per'),\n",
       "  (',', ',', 'O'),\n",
       "  ('said', 'VBD', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('gunmen', 'NNS', 'O'),\n",
       "  ('stopped', 'VBD', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('supply', 'NN', 'O'),\n",
       "  ('boat', 'NN', 'O'),\n",
       "  ('Wednesday', 'NNP', 'B-tim'),\n",
       "  ('as', 'IN', 'O'),\n",
       "  ('it', 'PRP', 'O'),\n",
       "  ('sailed', 'VBD', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('Delta', 'NNP', 'B-geo'),\n",
       "  ('State', 'NNP', 'I-geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('Bayelsa', 'NNP', 'B-geo'),\n",
       "  ('State', 'NNP', 'I-geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('inspect', 'VB', 'O'),\n",
       "  ('an', 'DT', 'O'),\n",
       "  ('offshore', 'JJ', 'O'),\n",
       "  ('oil', 'NN', 'O'),\n",
       "  ('field', 'NN', 'O'),\n",
       "  ('owned', 'VBN', 'O'),\n",
       "  ('by', 'IN', 'O'),\n",
       "  ('Royal-Dutch', 'NNP', 'B-org'),\n",
       "  ('Shell', 'NNP', 'I-org'),\n",
       "  ('.', '.', 'O')],\n",
       " [('The', 'DT', 'O'),\n",
       "  ('Muslim', 'NNP', 'B-org'),\n",
       "  ('Brotherhood', 'NNP', 'I-org'),\n",
       "  ('is', 'VBZ', 'O'),\n",
       "  ('banned', 'VBN', 'O'),\n",
       "  ('as', 'IN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('political', 'JJ', 'O'),\n",
       "  ('party', 'NN', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('but', 'CC', 'O'),\n",
       "  ('it', 'PRP', 'O'),\n",
       "  ('endorses', 'VBZ', 'O'),\n",
       "  ('so-called', 'JJ', 'O'),\n",
       "  ('independent', 'JJ', 'O'),\n",
       "  ('candidates', 'NNS', 'O'),\n",
       "  ('whose', 'WP$', 'O'),\n",
       "  ('allegiance', 'NN', 'O'),\n",
       "  ('to', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('party', 'NN', 'O'),\n",
       "  ('is', 'VBZ', 'O'),\n",
       "  ('known', 'VBN', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('voters', 'NNS', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('The', 'DT', 'O'),\n",
       "  ('German', 'JJ', 'B-gpe'),\n",
       "  ('firm', 'NN', 'O'),\n",
       "  ('works', 'VBZ', 'O'),\n",
       "  ('as', 'IN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('sub-contractor', 'NN', 'O'),\n",
       "  ('for', 'IN', 'O'),\n",
       "  ('Shell', 'NNP', 'B-org'),\n",
       "  ('.', '.', 'O')],\n",
       " [('The', 'DT', 'O'),\n",
       "  ('proposed', 'JJ', 'O'),\n",
       "  ('law', 'NN', 'O'),\n",
       "  ('calls', 'VBZ', 'O'),\n",
       "  ('for', 'IN', 'O'),\n",
       "  ('setting', 'VBG', 'O'),\n",
       "  ('up', 'RP', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('\"', '``', 'O'),\n",
       "  ('religious', 'JJ', 'O'),\n",
       "  ('police', 'NN', 'O'),\n",
       "  ('force', 'NN', 'O'),\n",
       "  ('\"', '``', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('make', 'VB', 'O'),\n",
       "  ('sure', 'JJ', 'O'),\n",
       "  ('people', 'NNS', 'O'),\n",
       "  ('adhere', 'VBP', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('Islamic', 'JJ', 'O'),\n",
       "  ('values', 'NNS', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('public', 'JJ', 'O'),\n",
       "  ('places', 'NNS', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('entertainment', 'NN', 'O'),\n",
       "  ('outlets', 'NNS', 'O'),\n",
       "  ('close', 'RB', 'O'),\n",
       "  ('during', 'IN', 'O'),\n",
       "  ('weekly', 'JJ', 'O'),\n",
       "  ('Friday', 'NNP', 'B-tim'),\n",
       "  ('prayers', 'NNS', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('In', 'IN', 'O'),\n",
       "  ('other', 'JJ', 'O'),\n",
       "  ('violence', 'NN', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('U.S.', 'NNP', 'B-gpe'),\n",
       "  ('officials', 'NNS', 'O'),\n",
       "  ('said', 'VBD', 'O'),\n",
       "  ('one', 'CD', 'O'),\n",
       "  ('American', 'JJ', 'B-gpe'),\n",
       "  ('soldier', 'NN', 'O'),\n",
       "  ('was', 'VBD', 'O'),\n",
       "  ('killed', 'VBN', 'O'),\n",
       "  ('while', 'IN', 'O'),\n",
       "  ('on', 'IN', 'O'),\n",
       "  ('patrol', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Baghdad', 'NNP', 'B-geo'),\n",
       "  ('Sunday', 'NNP', 'B-tim'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Police', 'NNS', 'O'),\n",
       "  ('put', 'VBD', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('number', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('marchers', 'NNS', 'O'),\n",
       "  ('at', 'IN', 'O'),\n",
       "  ('10,000', 'CD', 'O'),\n",
       "  ('while', 'IN', 'O'),\n",
       "  ('organizers', 'NNS', 'O'),\n",
       "  ('claimed', 'VBD', 'O'),\n",
       "  ('it', 'PRP', 'O'),\n",
       "  ('was', 'VBD', 'O'),\n",
       "  ('1,00,000', 'CD', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('The', 'DT', 'O'),\n",
       "  ('attacks', 'NNS', 'O'),\n",
       "  ('occurred', 'VBD', 'O'),\n",
       "  ('after', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('government', 'NN', 'O'),\n",
       "  ('said', 'VBD', 'O'),\n",
       "  ('it', 'PRP', 'O'),\n",
       "  ('will', 'MD', 'O'),\n",
       "  ('go', 'VB', 'O'),\n",
       "  ('ahead', 'RB', 'O'),\n",
       "  ('with', 'IN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('reconciliation', 'NN', 'O'),\n",
       "  ('conference', 'NN', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('which', 'WDT', 'O'),\n",
       "  ('more', 'JJR', 'O'),\n",
       "  ('than', 'IN', 'O'),\n",
       "  ('1,300', 'CD', 'O'),\n",
       "  ('Somali', 'JJ', 'B-gpe'),\n",
       "  ('elders', 'NNS', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('warlords', 'NNS', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('politicians', 'NNS', 'O'),\n",
       "  ('are', 'VBP', 'O'),\n",
       "  ('invited', 'VBN', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('The', 'DT', 'O'),\n",
       "  ('step', 'NN', 'O'),\n",
       "  ('will', 'MD', 'O'),\n",
       "  ('allow', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('facility', 'NN', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('operate', 'VB', 'O'),\n",
       "  ('at', 'IN', 'O'),\n",
       "  ('full', 'JJ', 'O'),\n",
       "  ('capacity', 'NN', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Violators', 'NNS', 'O'),\n",
       "  ('could', 'MD', 'O'),\n",
       "  ('be', 'VB', 'O'),\n",
       "  ('jailed', 'VBN', 'O'),\n",
       "  ('for', 'IN', 'O'),\n",
       "  ('up', 'RB', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('six', 'CD', 'O'),\n",
       "  ('months', 'NNS', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('British', 'JJ', 'B-gpe'),\n",
       "  ('police', 'NNS', 'O'),\n",
       "  ('say', 'VBP', 'O'),\n",
       "  ('they', 'PRP', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('arrested', 'VBN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('man', 'NN', 'O'),\n",
       "  ('who', 'WP', 'O'),\n",
       "  ('dressed', 'VBD', 'O'),\n",
       "  ('as', 'IN', 'O'),\n",
       "  ('suicide', 'NN', 'O'),\n",
       "  ('bomber', 'NN', 'O'),\n",
       "  ('at', 'IN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('demonstration', 'NN', 'O'),\n",
       "  ('against', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('publication', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('cartoons', 'NNS', 'O'),\n",
       "  ('depicting', 'VBG', 'O'),\n",
       "  ('Islam', 'NNP', 'B-org'),\n",
       "  (\"'s\", 'POS', 'O'),\n",
       "  ('Prophet', 'NNP', 'B-per'),\n",
       "  ('Muhammad', 'NNP', 'I-per'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Local', 'JJ', 'O'),\n",
       "  ('news', 'NN', 'O'),\n",
       "  ('reports', 'NNS', 'O'),\n",
       "  ('said', 'VBD', 'O'),\n",
       "  ('at', 'IN', 'O'),\n",
       "  ('least', 'JJS', 'O'),\n",
       "  ('five', 'CD', 'O'),\n",
       "  ('mortar', 'NN', 'O'),\n",
       "  ('shells', 'NNS', 'O'),\n",
       "  ('hit', 'VBD', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('palace', 'NN', 'O'),\n",
       "  ('compound', 'NN', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('other', 'JJ', 'O'),\n",
       "  ('mortars', 'NNS', 'O'),\n",
       "  ('were', 'VBD', 'O'),\n",
       "  ('fired', 'VBN', 'O'),\n",
       "  ('elsewhere', 'RB', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Mogadishu', 'NNP', 'B-geo'),\n",
       "  ('Wednesday', 'NNP', 'B-tim'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Iran', 'NNP', 'B-gpe'),\n",
       "  ('this', 'DT', 'O'),\n",
       "  ('week', 'NN', 'O'),\n",
       "  ('restarted', 'VBD', 'O'),\n",
       "  ('parts', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('conversion', 'NN', 'O'),\n",
       "  ('process', 'NN', 'O'),\n",
       "  ('at', 'IN', 'O'),\n",
       "  ('its', 'PRP$', 'O'),\n",
       "  ('Isfahan', 'NNP', 'B-geo'),\n",
       "  ('nuclear', 'JJ', 'O'),\n",
       "  ('plant', 'NN', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('The', 'DT', 'O'),\n",
       "  ('protest', 'NN', 'O'),\n",
       "  ('comes', 'VBZ', 'O'),\n",
       "  ('on', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('eve', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('annual', 'JJ', 'O'),\n",
       "  ('conference', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('Britain', 'NNP', 'B-geo'),\n",
       "  (\"'s\", 'POS', 'O'),\n",
       "  ('ruling', 'VBG', 'O'),\n",
       "  ('Labor', 'NNP', 'B-org'),\n",
       "  ('Party', 'NNP', 'I-org'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('southern', 'JJ', 'O'),\n",
       "  ('English', 'JJ', 'B-gpe'),\n",
       "  ('seaside', 'NN', 'O'),\n",
       "  ('resort', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('Brighton', 'NNP', 'B-geo'),\n",
       "  ('.', '.', 'O')],\n",
       " [('The', 'DT', 'O'),\n",
       "  ('London', 'NNP', 'B-geo'),\n",
       "  ('march', 'NN', 'O'),\n",
       "  ('came', 'VBD', 'O'),\n",
       "  ('ahead', 'RB', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('anti-war', 'JJ', 'O'),\n",
       "  ('protests', 'NNS', 'O'),\n",
       "  ('today', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('other', 'JJ', 'O'),\n",
       "  ('cities', 'NNS', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('including', 'VBG', 'O'),\n",
       "  ('Rome', 'NNP', 'B-geo'),\n",
       "  (',', ',', 'O'),\n",
       "  ('Paris', 'NNP', 'B-geo'),\n",
       "  (',', ',', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('Madrid', 'NNP', 'B-geo'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Officials', 'NNS', 'O'),\n",
       "  ('will', 'MD', 'O'),\n",
       "  ('not', 'RB', 'O'),\n",
       "  ('say', 'VB', 'O'),\n",
       "  ('how', 'WRB', 'O'),\n",
       "  ('many', 'JJ', 'O'),\n",
       "  ('troops', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('arrived', 'VBN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('Sunni', 'NNP', 'B-geo'),\n",
       "  ('Arab', 'NNP', 'I-geo'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('Kurdish', 'NNP', 'B-geo'),\n",
       "  ('city', 'NN', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('where', 'WRB', 'O'),\n",
       "  ('bombings', 'NNS', 'O'),\n",
       "  ('last', 'JJ', 'O'),\n",
       "  ('week', 'NN', 'O'),\n",
       "  ('killed', 'VBD', 'O'),\n",
       "  ('at', 'IN', 'O'),\n",
       "  ('least', 'JJS', 'O'),\n",
       "  ('34', 'CD', 'O'),\n",
       "  ('people', 'NNS', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('wounded', 'VBD', 'O'),\n",
       "  ('more', 'JJR', 'O'),\n",
       "  ('than', 'IN', 'O'),\n",
       "  ('200', 'CD', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('A', 'DT', 'O'),\n",
       "  ('six-party', 'JJ', 'O'),\n",
       "  ('coalition', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('religious', 'JJ', 'O'),\n",
       "  ('based', 'VBN', 'O'),\n",
       "  ('parties', 'NNS', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('Mutahida', 'NNP', 'B-org'),\n",
       "  ('Majlis-e-Amal', 'NNP', 'I-org'),\n",
       "  (',', ',', 'O'),\n",
       "  ('dominates', 'VBZ', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('provincial', 'JJ', 'O'),\n",
       "  ('assembly', 'NN', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('so', 'IN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('bill', 'NN', 'O'),\n",
       "  ('was', 'VBD', 'O'),\n",
       "  ('easily', 'RB', 'O'),\n",
       "  ('passed', 'VBN', 'O'),\n",
       "  ('Thursday', 'NNP', 'B-tim'),\n",
       "  ('by', 'IN', 'O'),\n",
       "  ('a', 'DT', 'O'),\n",
       "  ('vote', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('68-34', 'CD', 'O'),\n",
       "  ('.', '.', 'O')],\n",
       " [('Officials', 'NNS', 'O'),\n",
       "  ('say', 'VBP', 'O'),\n",
       "  ('al', 'NNP', 'B-org'),\n",
       "  ('Qaida', 'NNP', 'I-org'),\n",
       "  ('in', 'IN', 'I-org'),\n",
       "  ('Iraq', 'NNP', 'I-org'),\n",
       "  ('fighters', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('fled', 'VBN', 'O'),\n",
       "  ('successful', 'JJ', 'O'),\n",
       "  ('campaigns', 'NNS', 'O'),\n",
       "  ('against', 'IN', 'O'),\n",
       "  ('them', 'PRP', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Anbar', 'NNP', 'B-geo'),\n",
       "  ('province', 'NN', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('Baghdad', 'NNP', 'B-geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('other', 'JJ', 'O'),\n",
       "  ('northern', 'JJ', 'O'),\n",
       "  ('provinces', 'NNS', 'O'),\n",
       "  ('.', '.', 'O')]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Poor', 'JJ', 'O'),\n",
       " ('residents', 'NNS', 'O'),\n",
       " ('often', 'RB', 'O'),\n",
       " ('complain', 'VBP', 'O'),\n",
       " ('they', 'PRP', 'O')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting into Hugging Face desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to Hugging Face Datasets format\n",
    "def convert_to_dict(sentences):\n",
    "    words = [[word for word, pos, tag in sentence] for sentence in sentences]\n",
    "    pos_tags = [[pos for word, pos, tag in sentence] for sentence in sentences]\n",
    "    ner_tags = [[tag for word, pos, tag in sentence] for sentence in sentences]\n",
    "    return {\"tokens\": words, \"pos_tags\": pos_tags, \"ner_tags\": ner_tags}\n",
    "\n",
    "train_data = convert_to_dict(train_sentences)\n",
    "val_data = convert_to_dict(val_sentences)\n",
    "test_data = convert_to_dict(test_sentences)\n",
    "\n",
    "# Create a dataset dictionary\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict(train_data),\n",
    "    'validation': Dataset.from_dict(val_data),\n",
    "    'test': Dataset.from_dict(test_data)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 9\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 9\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Poor',\n",
       " 'residents',\n",
       " 'often',\n",
       " 'complain',\n",
       " 'they',\n",
       " 'have',\n",
       " 'been',\n",
       " 'cheated',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'huge',\n",
       " 'riches',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'their',\n",
       " 'tribal',\n",
       " 'lands',\n",
       " '-',\n",
       " 'where',\n",
       " 'the',\n",
       " 'bulk',\n",
       " 'of',\n",
       " 'Nigeria',\n",
       " \"'s\",\n",
       " '2.3',\n",
       " 'million',\n",
       " 'barrels',\n",
       " 'of',\n",
       " 'petroleum',\n",
       " 'are',\n",
       " 'pumped',\n",
       " 'daily',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"].features['ner_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbering the labels so that network can accept them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unique tags\n",
    "unique_tags = list(set(tag for doc in dataset_dict['train']['ner_tags'] for tag in doc))\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'B-org': 0,\n",
       "  'O': 1,\n",
       "  'I-org': 2,\n",
       "  'B-gpe': 3,\n",
       "  'I-art': 4,\n",
       "  'I-per': 5,\n",
       "  'B-per': 6,\n",
       "  'B-geo': 7,\n",
       "  'B-tim': 8,\n",
       "  'I-geo': 9,\n",
       "  'B-art': 10},\n",
       " {0: 'B-org',\n",
       "  1: 'O',\n",
       "  2: 'I-org',\n",
       "  3: 'B-gpe',\n",
       "  4: 'I-art',\n",
       "  5: 'I-per',\n",
       "  6: 'B-per',\n",
       "  7: 'B-geo',\n",
       "  8: 'B-tim',\n",
       "  9: 'I-geo',\n",
       "  10: 'B-art'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id,id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "# model_checkpoint = \"dslim/bert-base-NER\"\n",
    "# model_checkpoint=\"nlpso/m3_hierarchical_ner_ref_cmbert_iob2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109545a2dfdc4466a796e2051c62fc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5d2b2db2294031b7d530af174496c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c908bb00dc854210bca815a49f57a46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    #if labelize==True:\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(tag2id[label[word_idx]] if True else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        #labels.append(label)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "    \n",
    "# global labelize\n",
    "# labelize=False    \n",
    "tokenized_datasets = dataset_dict.map(tokenize_and_align_labels, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'pos_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'pos_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'pos_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring BitsandBytes and Lora (For future usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection, model training & logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/transformers/training_args.py:2101: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702399688dda48c7852db62d632353f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8681701ebf34b299128751da3d881c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.785125732421875, 'eval_precision': 0.02631578947368421, 'eval_recall': 0.041666666666666664, 'eval_f1': 0.03225806451612904, 'eval_accuracy': 0.7488789237668162, 'eval_runtime': 0.1992, 'eval_samples_per_second': 45.181, 'eval_steps_per_second': 5.02, 'epoch': 1.0}\n",
      "{'train_runtime': 3.1709, 'train_samples_per_second': 7.884, 'train_steps_per_second': 0.631, 'train_loss': 2.152837038040161, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n    # Log the model artifact\\n    trainer.save_model(os.path.join(\"results\", \"model\"))\\n    tokenizer.save_pretrained(os.path.join(\"results\", \"model\"))\\n\\n    mlflow.log_artifacts(\"results/model\")\\n\\n\\n    # Log other artifacts if needed\\n    # For example, logging training args\\n    with open(\"results/training_args.bin\", \"wb\") as f:\\n        torch.save(args, f)\\n    mlflow.log_artifact(\"results/training_args.bin\")\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Model\n",
    "model = AutoModelForTokenClassification.from_pretrained(pre_trained_model, num_labels=len(unique_tags), ignore_mismatched_sizes=True,\n",
    "                                                        #quantization_config=nf4_config\n",
    "                                                        )\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, len(unique_tags))\n",
    "model.num_labels = len(unique_tags)\n",
    "\n",
    "# Metrics\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    #\"test-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    use_mps_device=True,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# # Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #peft_config=peft_config\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "# trainer = SFTTrainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     peft_config=peft_config\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "#     # Log metrics to MLflow\n",
    "#     mlflow.log_metrics(results)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Log the model artifact\n",
    "    trainer.save_model(os.path.join(\"results\", \"model\"))\n",
    "    tokenizer.save_pretrained(os.path.join(\"results\", \"model\"))\n",
    "\n",
    "    mlflow.log_artifacts(\"results/model\")\n",
    "\n",
    "\n",
    "    # Log other artifacts if needed\n",
    "    # For example, logging training args\n",
    "    with open(\"results/training_args.bin\", \"wb\") as f:\n",
    "        torch.save(args, f)\n",
    "    mlflow.log_artifact(\"results/training_args.bin\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'LABEL_8', 'score': 0.15944915, 'word': 'Hugging', 'start': 0, 'end': 7}, {'entity_group': 'LABEL_1', 'score': 0.1302425, 'word': 'Face Inc', 'start': 8, 'end': 16}, {'entity_group': 'LABEL_9', 'score': 0.18290514, 'word': '. is a', 'start': 16, 'end': 22}, {'entity_group': 'LABEL_1', 'score': 0.20455617, 'word': 'company based', 'start': 23, 'end': 36}, {'entity_group': 'LABEL_2', 'score': 0.19775945, 'word': 'in', 'start': 37, 'end': 39}, {'entity_group': 'LABEL_3', 'score': 0.15408538, 'word': 'New', 'start': 40, 'end': 43}, {'entity_group': 'LABEL_7', 'score': 0.14170787, 'word': 'York City', 'start': 44, 'end': 53}, {'entity_group': 'LABEL_9', 'score': 0.19538349, 'word': '.', 'start': 53, 'end': 54}, {'entity_group': 'LABEL_1', 'score': 0.2051506, 'word': 'Its', 'start': 55, 'end': 58}, {'entity_group': 'LABEL_9', 'score': 0.19571926, 'word': 'headquarters', 'start': 59, 'end': 71}, {'entity_group': 'LABEL_1', 'score': 0.18712822, 'word': 'are in', 'start': 72, 'end': 78}, {'entity_group': 'LABEL_8', 'score': 0.12258852, 'word': 'D', 'start': 79, 'end': 80}, {'entity_group': 'LABEL_4', 'score': 0.14267759, 'word': '##UMBO', 'start': 80, 'end': 84}, {'entity_group': 'LABEL_9', 'score': 0.188729, 'word': ', therefore very', 'start': 84, 'end': 100}, {'entity_group': 'LABEL_1', 'score': 0.18587175, 'word': 'close', 'start': 101, 'end': 106}, {'entity_group': 'LABEL_2', 'score': 0.18156256, 'word': 'to', 'start': 107, 'end': 109}, {'entity_group': 'LABEL_1', 'score': 0.17642163, 'word': 'the', 'start': 110, 'end': 113}, {'entity_group': 'LABEL_6', 'score': 0.13209723, 'word': 'Manhattan', 'start': 114, 'end': 123}, {'entity_group': 'LABEL_4', 'score': 0.16813006, 'word': 'Bridge', 'start': 124, 'end': 130}, {'entity_group': 'LABEL_1', 'score': 0.17802809, 'word': '.', 'start': 130, 'end': 131}]\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\",device=device)\n",
    "sample_text = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\n",
    "ner_results = ner_pipeline(sample_text)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert numbered labels into NER tags using lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'B-geo', 'score': 0.15944915, 'word': 'Hugging', 'start': 0, 'end': 7}, {'entity_group': 'B-org', 'score': 0.1302425, 'word': 'Face Inc', 'start': 8, 'end': 16}, {'entity_group': 'B-tim', 'score': 0.18290514, 'word': '. is a', 'start': 16, 'end': 22}, {'entity_group': 'B-org', 'score': 0.20455617, 'word': 'company based', 'start': 23, 'end': 36}, {'entity_group': 'O', 'score': 0.19775945, 'word': 'in', 'start': 37, 'end': 39}, {'entity_group': 'I-org', 'score': 0.15408538, 'word': 'New', 'start': 40, 'end': 43}, {'entity_group': 'B-per', 'score': 0.14170787, 'word': 'York City', 'start': 44, 'end': 53}, {'entity_group': 'B-tim', 'score': 0.19538349, 'word': '.', 'start': 53, 'end': 54}, {'entity_group': 'B-org', 'score': 0.2051506, 'word': 'Its', 'start': 55, 'end': 58}, {'entity_group': 'B-tim', 'score': 0.19571926, 'word': 'headquarters', 'start': 59, 'end': 71}, {'entity_group': 'B-org', 'score': 0.18712822, 'word': 'are in', 'start': 72, 'end': 78}, {'entity_group': 'B-geo', 'score': 0.12258852, 'word': 'D', 'start': 79, 'end': 80}, {'entity_group': 'B-gpe', 'score': 0.14267759, 'word': '##UMBO', 'start': 80, 'end': 84}, {'entity_group': 'B-tim', 'score': 0.188729, 'word': ', therefore very', 'start': 84, 'end': 100}, {'entity_group': 'B-org', 'score': 0.18587175, 'word': 'close', 'start': 101, 'end': 106}, {'entity_group': 'O', 'score': 0.18156256, 'word': 'to', 'start': 107, 'end': 109}, {'entity_group': 'B-org', 'score': 0.17642163, 'word': 'the', 'start': 110, 'end': 113}, {'entity_group': 'I-per', 'score': 0.13209723, 'word': 'Manhattan', 'start': 114, 'end': 123}, {'entity_group': 'B-gpe', 'score': 0.16813006, 'word': 'Bridge', 'start': 124, 'end': 130}, {'entity_group': 'B-org', 'score': 0.17802809, 'word': '.', 'start': 130, 'end': 131}]\n"
     ]
    }
   ],
   "source": [
    "# Function to replace entity_group with corresponding key from the lookup\n",
    "def replace_entity_group(data, lookup):\n",
    "    # Create a reverse lookup dictionary to map indices to their labels\n",
    "    reverse_lookup = {v: k for k, v in lookup.items()}\n",
    "    # Update each dictionary in the list\n",
    "    for item in data:\n",
    "        entity_group_value = int(item['entity_group'].split('_')[1]) - 1  # Extract the number and adjust to zero-indexed\n",
    "        item['entity_group'] = reverse_lookup.get(entity_group_value, item['entity_group'])\n",
    "    return data\n",
    "\n",
    "lookup=tag2id\n",
    "data=ner_results\n",
    "\n",
    "# Replace entity_group in the data\n",
    "updated_data = replace_entity_group(data, lookup)\n",
    "\n",
    "# Print the updated data\n",
    "print(updated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity_group': 'B-geo',\n",
       " 'score': 0.15944915,\n",
       " 'word': 'Hugging',\n",
       " 'start': 0,\n",
       " 'end': 7}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-org': 0,\n",
       " 'O': 1,\n",
       " 'I-org': 2,\n",
       " 'B-gpe': 3,\n",
       " 'I-art': 4,\n",
       " 'I-per': 5,\n",
       " 'B-per': 6,\n",
       " 'B-geo': 7,\n",
       " 'B-tim': 8,\n",
       " 'I-geo': 9,\n",
       " 'B-art': 10}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signature Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Predict with the model\n",
    "tuned_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\",device=device)\n",
    "input_example = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\n",
    "# ner_results = ner_pipeline(sample_text)\n",
    "# print(ner_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Infer the model signature, including a representative input, the expected output, and the parameters that we would like to be able to override at inference time.\n",
    "signature = mlflow.models.infer_signature(\n",
    "    [\"This is a test!\", \"And this is also a test.\"],\n",
    "    mlflow.transformers.generate_signature_output(\n",
    "        tuned_pipeline, [\"This is a test response!\", \"So is this.\"]\n",
    "    ),\n",
    "    #params=model_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the pipeline to the existing training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/th/h0zk_0nj49j9y07r3bd6xgb00000gn/T/ipykernel_1156/2330167736.py:3: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.34.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  model_info = mlflow.transformers.log_model(\n",
      "/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/mlflow/models/model.py:619: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.34.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
      "/opt/anaconda3/envs/gen_3.8/lib/python3.8/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "#Log the pipeline to the existing training run\n",
    "with mlflow.start_run(run_id=run.info.run_id):\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=tuned_pipeline,\n",
    "        #artifact_path=\"fine_tuned\",\n",
    "        artifact_path=\"model\",\n",
    "        signature=signature,\n",
    "        input_example=[\"Pass in a string\", \"And have it mark as spam or not.\"],\n",
    "        #model_config=model_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/th/h0zk_0nj49j9y07r3bd6xgb00000gn/T/ipykernel_1156/2856011991.py:2: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.34.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  loaded = mlflow.transformers.load_model(model_uri=model_info.model_uri)\n",
      "2024/06/23 01:45:54 INFO mlflow.transformers: 'runs:/e8b2a54edc2c46a4b4f83ec458021a4d/model' resolved as 'file:///Users/lukishyadav/Desktop/engineering/case_studies/ner_casestudy/mlruns/871896506229020652/e8b2a54edc2c46a4b4f83ec458021a4d/artifacts/model'\n",
      "2024/06/23 01:45:54 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_1',\n",
       "  'score': 0.18289715,\n",
       "  'index': 1,\n",
       "  'word': 'Hello',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.18411833,\n",
       "  'index': 2,\n",
       "  'word': '!',\n",
       "  'start': 5,\n",
       "  'end': 6},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.17381199,\n",
       "  'index': 3,\n",
       "  'word': 'there',\n",
       "  'start': 7,\n",
       "  'end': 12}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load our saved model in the native transformers format\n",
    "loaded = mlflow.transformers.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "# Define a test example that we expect to be classified as spam\n",
    "validation_text = (\n",
    "    \"Want to learn how to make MILLIONS with no effort? Click HERE now! See for yourself! Guaranteed to make you instantly rich! \"\n",
    "    \"Don't miss out you could be a winner!\"\n",
    ")\n",
    "\n",
    "validation_text=(\"Hello! there\")\n",
    "\n",
    "# validate the performance of our fine-tuning\n",
    "loaded(validation_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FInal inference testing of invocations api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    # Already added when you pass json=\n",
    "    # 'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "json_data = {\n",
    "    'inputs': [\n",
    "        'Hello',\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.post('http://en.wikipedia.org/curl', headers=headers, json=json_data)\n",
    "response = requests.post('http://127.0.0.1:5000/invocations', headers=headers, json=json_data)\n",
    "response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
